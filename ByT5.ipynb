{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14015625,"sourceType":"datasetVersion","datasetId":8928722},{"sourceId":14015638,"sourceType":"datasetVersion","datasetId":8928732}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# this is the research paper this algorithm is adopted from [https://arxiv.org/abs/2303.14588]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:16:05.190715Z","iopub.execute_input":"2025-12-17T09:16:05.191074Z","iopub.status.idle":"2025-12-17T09:16:05.494974Z","shell.execute_reply.started":"2025-12-17T09:16:05.191040Z","shell.execute_reply":"2025-12-17T09:16:05.494216Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/val-nlp-project/val.txt\n/kaggle/input/train-nlp-project/train.txt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 1: CHECK GPU\n# Run this to know which GPU Kaggle assigned you.\n# If nvidia-smi is absent, Kaggle might not have GPU enabled for the session.\nimport os\ngpu_info = !nvidia-smi -L || true\nprint(\"GPU info (if available):\")\nprint(\"\\n\".join(gpu_info))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:16:07.793104Z","iopub.execute_input":"2025-12-17T09:16:07.793585Z","iopub.status.idle":"2025-12-17T09:16:07.823578Z","shell.execute_reply.started":"2025-12-17T09:16:07.793557Z","shell.execute_reply":"2025-12-17T09:16:07.822840Z"}},"outputs":[{"name":"stdout","text":"GPU info (if available):\nGPU 0: Tesla P100-PCIE-16GB (UUID: GPU-23ca7607-bd66-1559-44ce-ac0666ec0c8f)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 2: Install required packages \n!pip install -q \\\n    pyarrow==19.0.0 \\\n    datasets==2.14.7 \\\n    transformers==4.35.2 \\\n    accelerate==0.24.1 \\\n    peft==0.7.1 \\\n    sentencepiece \\\n    evaluate\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:14:35.658581Z","iopub.execute_input":"2025-12-17T09:14:35.658907Z","iopub.status.idle":"2025-12-17T09:14:55.148070Z","shell.execute_reply.started":"2025-12-17T09:14:35.658880Z","shell.execute_reply":"2025-12-17T09:14:55.147172Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Imports and device setup\nimport os\nimport re\nimport math\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport torch\nfrom datasets import Dataset, DatasetDict\nimport evaluate\n\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nmetric = evaluate.load(\"accuracy\") \n# Device (torch) — Trainer uses this internally but it's useful\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:16:11.156050Z","iopub.execute_input":"2025-12-17T09:16:11.156365Z","iopub.status.idle":"2025-12-17T09:16:38.704627Z","shell.execute_reply.started":"2025-12-17T09:16:11.156335Z","shell.execute_reply":"2025-12-17T09:16:38.703902Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n2025-12-17 09:16:20.108139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765962980.295789      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765962980.348116      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1765962980.820844      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1765962980.820894      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1765962980.820896      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1765962980.820899      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc367ee2f9c4496987c82960497a27a6"}},"metadata":{}},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# importance of normalization : Convert everything to the standard Arabic set so the model learns consistently.\n# Cell 4: Unicode diacritics removal and normalization helpers\n\n# Arabic diacritics (Tashkeel) common ranges we remove:\n\nimport re\nimport unicodedata\n\n# Arabic diacritics (Harakat)\nDIACRITICS = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n\ndef remove_diacritics(text: str) -> str:\n    \"\"\"Remove Arabic diacritics (tashkeel) from text.\"\"\"\n    return DIACRITICS.sub(\"\", text)\n\ndef normalize_text_advanced(text):\n    # ---------------------------------------------------------\n    # 0. Unicode Normalize (very important)\n    # ---------------------------------------------------------\n    text = unicodedata.normalize(\"NFKC\", text)\n\n    # ---------------------------------------------------------\n    # 1. Fix common Arabic ligatures (ﻻ → لا)\n    # ---------------------------------------------------------\n    text = text.replace(\"\\uFEFB\", \"لا\").replace(\"\\uFEFC\", \"لا\")  # Lam-Alef ligatures\n\n    # ---------------------------------------------------------\n    # 2. Remove Tatweel\n    # ---------------------------------------------------------\n    text = text.replace(\"ـ\", \"\")\n\n    # ---------------------------------------------------------\n    # 3. Normalize letter shapes\n    # ---------------------------------------------------------\n    replacements = {\n        \"\\u0622\": \"ا\",  # Alef with madda\n        \"\\u0623\": \"ا\",  # Alef with hamza above\n        \"\\u0625\": \"ا\",  # Alef with hamza below\n        \"\\u0671\": \"ا\",  # Alef wasla\n        \"\\u0649\": \"ي\",  # Alef Maqsura → Yeh\n        \"\\u06CC\": \"ي\",  # Persian Yeh → Arabic Yeh\n        \"\\u0643\": \"ك\",  # Normalize Kaf\n        \"\\u06A9\": \"ك\",  # Persian Kaf → Arabic Kaf\n    }\n    for k, v in replacements.items():\n        text = text.replace(k, v)\n\n    # ---------------------------------------------------------\n    # 4. Remove page number references like ( 21 / 227 )\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\(\\s*\\d+\\s*/\\s*\\d+\\s*\\)\", \" \", text)\n\n    # ---------------------------------------------------------\n    # 5. Remove pure numeric brackets (e.g., ( 48 ))\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\(\\s*\\d+\\s*\\)\", \" \", text)\n\n    # ---------------------------------------------------------\n    # 6. Remove content in brackets when it is clearly commentary\n    #    Example: ( قَوْلُهُ : كَفَى )\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\(\\s*[^()]*?\\d+[^()]*?\\)\", \" \", text)  # commentary w/ numbers\n    # Optional: remove ALL bracketed commentary (if you want)\n    # text = re.sub(r\"[\\(\\[][^()\\[\\]]*[\\)\\]]\", \" \", text)\n\n    # ---------------------------------------------------------\n    # 7. Remove double diacritics (very important)\n    # ---------------------------------------------------------\n    text = re.sub(f\"({DIACRITICS})+\", r\"\\1\", text)\n\n    # ---------------------------------------------------------\n    # 8. Remove weird invisible characters\n    # ---------------------------------------------------------\n    invisible_chars = [\n        \"\\u200C\",  # Zero-width non-joiner\n        \"\\u200D\",  # Zero-width joiner\n        \"\\u200E\",  # Left-to-right mark\n        \"\\u200F\",  # Right-to-left mark\n        \"\\u202A\", \"\\u202B\", \"\\u202C\", \"\\u202D\", \"\\u202E\",\n    ]\n    for ch in invisible_chars:\n        text = text.replace(ch, \"\")\n\n    # ---------------------------------------------------------\n    # 9. Fix spacing around punctuation\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\s+([.,؛،؟])\", r\"\\1\", text)  # no space before punctuation\n    text = re.sub(r\"([.,؛،؟])\\s*\", r\"\\1 \", text)  # one space after punctuation\n\n    # ---------------------------------------------------------\n    # 10. Collapse multiple spaces\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    return text.strip()\n\n# Quick sanity test:\nsample = \"وَلَوْ جَمَعَ ثُمَّ عَلِمَ\"\nprint(\"orig:\", sample)\nprint(\"no diac:\", remove_diacritics(sample))\nprint(\"norm:\", normalize_text_advanced(remove_diacritics(sample)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:17:48.039127Z","iopub.execute_input":"2025-12-17T09:17:48.040284Z","iopub.status.idle":"2025-12-17T09:17:48.051428Z","shell.execute_reply.started":"2025-12-17T09:17:48.040250Z","shell.execute_reply":"2025-12-17T09:17:48.050680Z"}},"outputs":[{"name":"stdout","text":"orig: وَلَوْ جَمَعَ ثُمَّ عَلِمَ\nno diac: ولو جمع ثم علم\nnorm: ولو جمع ثم علم\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\n# Cell 5: Load files and create pairs (input = undiacritized, target = original)\n# Put your train.txt and val.txt into the Kaggle working directory or dataset input path.\n\n\nTRAIN_PATH = \"/kaggle/input/train-nlp-project/train.txt\"\nVAL_PATH = \"/kaggle/input/val-nlp-project/val.txt\"\n\ndef load_pairs_from_file(path):\n    inputs = []\n    targets = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            target = line\n            src = remove_diacritics(line)\n            src = normalize_text_advanced(src)\n            # optional: add bos/eos tokens? ByT5's tokenizer handles that\n            inputs.append(src)\n            targets.append(target)\n    return inputs, targets\n\ntrain_srcs, train_tgts = load_pairs_from_file(TRAIN_PATH)\nval_srcs, val_tgts = load_pairs_from_file(VAL_PATH)\n\nprint(\"Loaded samples:\", len(train_srcs), \"train,\", len(val_srcs), \"val\")\nprint(\"example input -> target:\")\nprint(train_srcs[0])\nprint(train_tgts[0])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:17:53.856613Z","iopub.execute_input":"2025-12-17T09:17:53.856963Z","iopub.status.idle":"2025-12-17T09:17:57.802795Z","shell.execute_reply.started":"2025-12-17T09:17:53.856935Z","shell.execute_reply":"2025-12-17T09:17:57.802177Z"}},"outputs":[{"name":"stdout","text":"Loaded samples: 50000 train, 2500 val\nexample input -> target:\nولو جمع ثم علم ترك ركن من الاولي بطلتا ويعيدهما جامعا، او من الثانية، فان لم يطل تدارك، والا فباطلة ولا جمع، ولو جهل اعادهما لوقتيهما\nوَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ، أَوْ مِنْ الثَّانِيَةِ ، فَإِنْ لَمْ يَطُلْ تَدَارَكَ ، وَإِلَّا فَبَاطِلَةٌ وَلَا جَمَعَ ، وَلَوْ جَهِلَ أَعَادَهُمَا لِوَقْتَيْهِمَا\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nfinal_inputs = train_srcs \nfinal_targets = train_tgts \n\n# Cell 6: Make HuggingFace datasets\ntrain_dataset = Dataset.from_dict({\n    \"input_text\": final_inputs,\n    \"target_text\": final_targets\n})\n\nval_dataset = Dataset.from_dict({\n    \"input_text\": val_srcs,\n    \"target_text\": val_tgts\n})\n\ndatasets = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset\n})\n\nprint(datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:18:02.045489Z","iopub.execute_input":"2025-12-17T09:18:02.046216Z","iopub.status.idle":"2025-12-17T09:18:02.399076Z","shell.execute_reply.started":"2025-12-17T09:18:02.046186Z","shell.execute_reply":"2025-12-17T09:18:02.398413Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 50000\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 2500\n    })\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\n# Cell 7: initialize ByT5 tokenizer and model\nMODEL_NAME = \"google/byt5-small\"  # small version (recommended start)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)  # ByT5 uses byte-level tokenization\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\nprint(\"Loaded model:\", MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:18:05.410052Z","iopub.execute_input":"2025-12-17T09:18:05.410314Z","iopub.status.idle":"2025-12-17T09:18:17.342017Z","shell.execute_reply.started":"2025-12-17T09:18:05.410292Z","shell.execute_reply":"2025-12-17T09:18:17.341353Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72bd4d965464eb2abb5cd688236a1e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2f3b077f154c6fbce2960c1ac16ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509f3aaf62b14853a1016d7a912a58af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91290f38a5b40e198c30a767e28369b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa1dee1802049cf9dc21118a0154672"}},"metadata":{}},{"name":"stdout","text":"Loaded model: google/byt5-small\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# # Cell 8: Tokenize examples for seq2seq training\n# # We need to tokenize inputs and targets; set reasonable max lengths.\n\nMAX_INPUT_LENGTH = 512  # ByT5 uses bytes; 512 is safe for sentences\nMAX_TARGET_LENGTH = 512\n\n# def convert_to_tensors(batch):\n#     # Convert everything to torch.tensor\n#     batch = {k: torch.tensor(v) for k, v in batch.items()}\n\n#     # Mask padding in labels\n#     batch['labels'][batch['labels'] == tokenizer.pad_token_id] = -100\n#     return batch\n    \ndef preprocess_batch(examples):\n    # examples: dict with lists input_text, target_text\n    model_inputs = tokenizer(\n        examples[\"input_text\"],\n        max_length=MAX_INPUT_LENGTH,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    # Tokenize targets (labels)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"target_text\"],\n            max_length=MAX_TARGET_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\n# Step 1: tokenize inputs and targets\ntokenized = datasets.map(preprocess_batch, batched=True, remove_columns=datasets[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:18:24.953575Z","iopub.execute_input":"2025-12-17T09:18:24.954118Z","iopub.status.idle":"2025-12-17T09:19:07.512287Z","shell.execute_reply.started":"2025-12-17T09:18:24.954089Z","shell.execute_reply":"2025-12-17T09:19:07.511734Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdb821a60ff849489ea649dd343296e6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0798f9f561e4286bbcc09d130653b15"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"all_labels = []\nfor example in tokenized[\"train\"]:\n    all_labels.extend(example[\"labels\"])\n\nprint(\"Max label ID:\", max(all_labels))\nprint(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:21:17.482053Z","iopub.execute_input":"2025-12-17T09:21:17.482370Z","iopub.status.idle":"2025-12-17T09:21:22.350664Z","shell.execute_reply.started":"2025-12-17T09:21:17.482340Z","shell.execute_reply":"2025-12-17T09:21:22.349880Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 9: Data collator and metrics\n\nfrom transformers import DataCollatorForSeq2Seq\nfrom evaluate import load   \n# no more load_metric\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100  # this is important\n)\n\n\n# Load WER metric from evaluate\nwer_metric = load(\"wer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:22:33.544313Z","iopub.execute_input":"2025-12-17T09:22:33.545055Z","iopub.status.idle":"2025-12-17T09:22:35.115681Z","shell.execute_reply.started":"2025-12-17T09:22:33.545017Z","shell.execute_reply":"2025-12-17T09:22:35.114973Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358b11143d324798b32f9b343715f528"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Cell 10: DER implementation (character-level)\n# DER = (# incorrect diacritic characters predicted) / (# characters that should have diacritics) * 100\n\ndef calc_der_for_pair(gold: str, pred: str):\n    \"\"\"Compute counts for one pair: returns (total_diacritics_in_gold, wrong_diacritics_predicted).\"\"\"\n    # We'll iterate over gold and pred by base characters.\n    # Simpler approach: strip diacritics to get base char sequence, then align.\n    gold_bases = remove_diacritics(gold)\n    pred_bases = remove_diacritics(pred)\n\n    # If base lengths differ, align via a simple heuristic (we'll fallback to char-level min length)\n    L = min(len(gold_bases), len(pred_bases))\n    total_diac = 0\n    wrong = 0\n\n    gi = 0\n    pi = 0\n    # iterate over positions in bases (simple alignment)\n    for k in range(L):\n        # find indices in original strings that correspond to the k-th base char\n        # This is a little code-heavy but robust enough.\n        # Build lists of base char indices once (optimization skipped)\n        pass\n\n# Simpler robust implementation: expand each string into sequence of (base_char, diacritic_str)\ndef expand_to_bases_and_diacritics(s: str):\n    base_chars = []\n    diacritic_strs = []\n    current_base = None\n    current_diacs = []\n    for ch in s:\n        if DIACRITICS.match(ch):\n            if current_base is not None:\n                current_diacs.append(ch)\n            # else stray diacritic before base? ignore\n        else:\n            # new base char\n            if current_base is not None:\n                base_chars.append(current_base)\n                diacritic_strs.append(\"\".join(current_diacs))\n            current_base = ch\n            current_diacs = []\n    if current_base is not None:\n        base_chars.append(current_base)\n        diacritic_strs.append(\"\".join(current_diacs))\n    return base_chars, diacritic_strs\n\ndef der_counts(gold: str, pred: str):\n    gb, gd = expand_to_bases_and_diacritics(gold)\n    pb, pd = expand_to_bases_and_diacritics(pred)\n    L = min(len(gb), len(pb))\n    total_diac = 0\n    wrong = 0\n    for i in range(L):\n        gold_di = gd[i]\n        pred_di = pd[i]\n        if gold_di != \"\":\n            total_diac += 1\n            if gold_di != pred_di:\n                wrong += 1\n    # if gold has extra chars beyond pred, count their diacritics as errors\n    if len(gb) > L:\n        for i in range(L, len(gb)):\n            if gd[i] != \"\":\n                total_diac += 1\n                wrong += 1\n    return total_diac, wrong\n\n# Quick check\ng = \"وَلَوْ جَمَعَ\"\np = \"ولو جمع\"\nprint(\"DER counts:\", der_counts(g, p))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:22:40.220157Z","iopub.execute_input":"2025-12-17T09:22:40.220891Z","iopub.status.idle":"2025-12-17T09:22:40.230043Z","shell.execute_reply.started":"2025-12-17T09:22:40.220860Z","shell.execute_reply":"2025-12-17T09:22:40.229253Z"}},"outputs":[{"name":"stdout","text":"DER counts: (6, 6)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Cell 11: Training arguments\n# Adjust batch size depending on GPU:\n# - If T4: per_device_train_batch_size = 4 (or 2) \n# - If P100/V100: can try 8 or 16 for small\n# We'll use gradient accumulation to simulate larger effective batch.\n\nGPU_INFO = !nvidia-smi -L || true\ngpu_str = \"\\n\".join(GPU_INFO)\nprint(\"GPU:\", gpu_str)\n\n# Heuristics:\nif \"V100\" in gpu_str or \"A100\" in gpu_str or \"P100\" in gpu_str:\n    per_device_train_batch_size = 4\nelse:\n    per_device_train_batch_size = 2  # safer for T4\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"byt5_diacritizer\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_train_batch_size,\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,        # evaluate every 500 steps (tune for dataset size)\n    logging_steps=100,\n    save_total_limit=3,\n    save_steps=500,\n    num_train_epochs=5,\n    learning_rate=3e-4,    # ByT5 often likes 1e-4 - 5e-4; tune if needed\n    weight_decay=0.01,\n    warmup_steps=500,\n    fp16=False,             # use mixed precision if GPU supports\n    bf16=False,    \n    gradient_accumulation_steps=4,  # increase effective batch\n    gradient_checkpointing=True,           # Saves ~30 to 40% memory\n    load_best_model_at_end=True,\n    metric_for_best_model=\"char_acc\",\n    greater_is_better=True,\n    eval_accumulation_steps=4,\n    generation_max_length=MAX_TARGET_LENGTH,\n    report_to=\"none\",\n)\nprint(training_args)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:22:56.485626Z","iopub.execute_input":"2025-12-17T09:22:56.486374Z","iopub.status.idle":"2025-12-17T09:22:56.525064Z","shell.execute_reply.started":"2025-12-17T09:22:56.486344Z","shell.execute_reply":"2025-12-17T09:22:56.524472Z"}},"outputs":[{"name":"stdout","text":"GPU: GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-23ca7607-bd66-1559-44ce-ac0666ec0c8f)\nSeq2SeqTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=4,\neval_delay=0,\neval_steps=500,\nevaluation_strategy=IntervalStrategy.STEPS,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngeneration_config=None,\ngeneration_max_length=512,\ngeneration_num_beams=None,\ngradient_accumulation_steps=4,\ngradient_checkpointing=True,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=True,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=HubStrategy.EVERY_SAVE,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0003,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=byt5_diacritizer/runs/Dec17_09-22-56_f1ea0d457a97,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=100,\nlogging_strategy=IntervalStrategy.STEPS,\nlr_scheduler_type=SchedulerType.LINEAR,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=char_acc,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=5,\noptim=OptimizerNames.ADAMW_TORCH,\noptim_args=None,\noutput_dir=byt5_diacritizer,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\npredict_with_generate=True,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=[],\nresume_from_checkpoint=None,\nrun_name=byt5_diacritizer,\nsave_on_each_node=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=IntervalStrategy.STEPS,\nsave_total_limit=3,\nseed=42,\nskip_memory_metrics=True,\nsortish_sampler=False,\nsplit_batches=False,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=500,\nweight_decay=0.01,\n)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 12: compute_metrics function for Trainer\nimport numpy as np\nfrom transformers import EvalPrediction\n\ndef postprocess_text(preds, labels):\n    # Labels may contain -100. Replace with pad token id for decoding\n    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n    # decode token ids to strings\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return decoded_preds, decoded_labels\n\ndef compute_metrics(eval_preds):\n    # eval_preds: (pred_ids, labels_ids, (maybe) metrics)\n    preds_ids, labels_ids = eval_preds\n    if isinstance(preds_ids, tuple):\n        preds_ids = preds_ids[0]\n    # decode\n    decoded_preds, decoded_labels = postprocess_text(preds_ids, labels_ids)\n    \n    # compute simple token-level metrics: wer\n    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    \n    # compute DER\n    total_diac = 0\n    wrong_diac = 0\n    total_chars = 0\n    correct_chars = 0\n    for g, p in zip(decoded_labels, decoded_preds):\n        td, wd = der_counts(g, p)\n        total_diac += td\n        wrong_diac += wd\n        # simple char accuracy (consider base chars only)\n        gb, gd = expand_to_bases_and_diacritics(g)\n        pb, pd = expand_to_bases_and_diacritics(p)\n        L = min(len(g_bases), len(p_bases))\n        for i in range(L):\n            total_chars += 1\n            if g_bases[i] == p_bases[i] and ( (gd := expand_to_bases_and_diacritics(g)[1][i]) == (pd := expand_to_bases_and_diacritics(p)[1][i]) ):\n                correct_chars += 1\n    der = 100.0 * wrong_diac / total_diac if total_diac > 0 else 0.0\n    char_acc = 100.0 * correct_chars / total_chars if total_chars > 0 else 0.0\n\n    return {\n        \"wer\": wer,\n        \"DER\": der,\n        \"char_acc\": char_acc,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:23:01.575554Z","iopub.execute_input":"2025-12-17T09:23:01.576268Z","iopub.status.idle":"2025-12-17T09:23:01.583679Z","shell.execute_reply.started":"2025-12-17T09:23:01.576236Z","shell.execute_reply":"2025-12-17T09:23:01.583027Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"example = tokenized['train'][0]  # pick first example\nprint(\"Input IDs type:\", type(example['input_ids']))\nprint(\"Input IDs length:\", len(example['input_ids']))\nprint(\"Labels length:\", len(example['labels']))\ndecoded_input = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\ndecoded_label = tokenizer.decode(example['labels'], skip_special_tokens=True)\nprint(\"Decoded input:\", decoded_input)\nprint(\"Decoded target:\", decoded_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:23:12.267894Z","iopub.execute_input":"2025-12-17T09:23:12.268553Z","iopub.status.idle":"2025-12-17T09:23:12.412856Z","shell.execute_reply.started":"2025-12-17T09:23:12.268523Z","shell.execute_reply":"2025-12-17T09:23:12.412142Z"}},"outputs":[{"name":"stdout","text":"Input IDs type: <class 'list'>\nInput IDs length: 512\nLabels length: 512\nDecoded input: ولو جمع ثم علم ترك ركن من الاولي بطلتا ويعيدهما جامعا، او من الثانية، فان لم يطل تدارك، والا فباطلة ولا جمع، ولو جهل اعادهما لوقتيهما\nDecoded target: وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ، أَوْ مِنْ الثَّانِيَةِ ، فَإِنْ لَمْ يَطُلْ تَدَارَكَ ، وَإِلَّا فَبَاطِلَةٌ وَلَا جَمَعَ ، وَلَوْ جَهِلَ أَعَادَهُمَا لِوَقْتَيْهِمَا\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(tokenizer.pad_token, tokenizer.pad_token_id)\nprint(tokenized[\"train\"][1][\"labels\"])\n\nprint(tokenizer.unk_token_id)\nprint(len(tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:23:15.322253Z","iopub.execute_input":"2025-12-17T09:23:15.322986Z","iopub.status.idle":"2025-12-17T09:23:15.328616Z","shell.execute_reply.started":"2025-12-17T09:23:15.322956Z","shell.execute_reply":"2025-12-17T09:23:15.327836Z"}},"outputs":[{"name":"stdout","text":"<pad> 0\n[220, 133, 220, 145, 219, 170, 220, 135, 220, 145, 35, 219, 166, 220, 145, 219, 171, 220, 146, 220, 139, 35, 219, 181, 220, 145, 220, 141, 220, 149, 219, 178, 220, 144, 35, 219, 166, 220, 145, 220, 138, 220, 149, 220, 135, 220, 146, 35, 219, 173, 220, 147, 220, 138, 220, 145, 219, 170, 220, 136, 220, 145, 219, 172, 220, 145, 35, 220, 141, 220, 146, 219, 167, 220, 145, 220, 137, 220, 148, 220, 147, 219, 174, 220, 146, 220, 139, 220, 137, 220, 145, 35, 219, 170, 220, 135, 220, 149, 219, 188, 220, 145, 219, 185, 220, 146, 219, 178, 220, 145, 35, 220, 139, 220, 145, 219, 171, 220, 145, 220, 137, 220, 146, 220, 139, 35, 219, 173, 220, 145, 220, 136, 220, 147, 220, 141, 220, 136, 220, 144, 35, 220, 141, 220, 146, 219, 179, 220, 145, 220, 134, 220, 148, 220, 147, 219, 180, 220, 146, 220, 139, 220, 137, 220, 145, 35, 219, 143, 35, 220, 139, 220, 145, 219, 170, 220, 135, 220, 149, 219, 175, 220, 145, 220, 136, 220, 149, 219, 188, 220, 146, 35, 219, 166, 220, 145, 219, 188, 220, 149, 219, 185, 220, 146, 219, 178, 220, 143, 35, 220, 139, 220, 145, 219, 166, 220, 145, 219, 188, 220, 149, 219, 185, 220, 145, 219, 170, 219, 178, 220, 143, 35, 220, 136, 220, 147, 219, 174, 220, 149, 220, 135, 220, 146, 35, 219, 166, 220, 145, 220, 132, 220, 149, 220, 135, 220, 146, 219, 182, 220, 144, 35, 220, 139, 220, 145, 219, 166, 220, 145, 220, 133, 220, 149, 220, 132, 220, 145, 219, 170, 220, 135, 220, 144, 43, 35, 53, 55, 35, 50, 35, 54, 56, 56, 35, 44, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n2\n384\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Cell 13: Create Seq2SeqTrainer and train\n\noutput_dir = \"/kaggle/working/byt5_diacritizer\"\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Auto resume if a checkpoint already exists\n# Automatically resumes if a checkpoint exists in output_dir\ntrainer.train()\n\n\n# Save final state\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:23:22.795022Z","iopub.execute_input":"2025-12-17T09:23:22.795312Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    5/15625 00:10 < 14:46:05, 0.29 it/s, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Save and quick test\nbest_model_path = \"byt5_diacritizer/best_model\"\ntrainer.save_model(best_model_path)\nprint(\"Saved best model to\", best_model_path)\n\n# Simple inference function\nfrom transformers import pipeline\npipe = pipeline(\"text2text-generation\", model=best_model_path, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef diacritize(text: str, max_length: int = 512):\n    src = normalize_text_advanced(remove_diacritics(text))\n    out = pipe(src, max_length=max_length, do_sample=False)[0][\"generated_text\"]\n    return out\n\n# Example:\nprint(\"Input:\", \"ولو جمع ثم علم ترك ركن\")\nprint(\"Output:\", diacritize(\"وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:03:01.722203Z","iopub.status.idle":"2025-12-17T09:03:01.722489Z","shell.execute_reply.started":"2025-12-17T09:03:01.722370Z","shell.execute_reply":"2025-12-17T09:03:01.722385Z"}},"outputs":[],"execution_count":null}]}