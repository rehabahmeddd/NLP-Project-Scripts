{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14015625,"sourceType":"datasetVersion","datasetId":8928722},{"sourceId":14015638,"sourceType":"datasetVersion","datasetId":8928732}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# this is the research paper this algorithm is adopted from [https://arxiv.org/abs/2303.14588]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:03.840199Z","iopub.execute_input":"2025-12-17T10:50:03.840421Z","iopub.status.idle":"2025-12-17T10:50:04.123005Z","shell.execute_reply.started":"2025-12-17T10:50:03.840396Z","shell.execute_reply":"2025-12-17T10:50:04.122188Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/val-nlp-project/val.txt\n/kaggle/input/train-nlp-project/train.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1: CHECK GPU\n# Run this to know which GPU Kaggle assigned you.\n# If nvidia-smi is absent, Kaggle might not have GPU enabled for the session.\nimport os\ngpu_info = !nvidia-smi -L || true\nprint(\"GPU info (if available):\")\nprint(\"\\n\".join(gpu_info))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:04.124011Z","iopub.execute_input":"2025-12-17T10:50:04.124321Z","iopub.status.idle":"2025-12-17T10:50:04.154429Z","shell.execute_reply.started":"2025-12-17T10:50:04.124299Z","shell.execute_reply":"2025-12-17T10:50:04.153667Z"}},"outputs":[{"name":"stdout","text":"GPU info (if available):\nGPU 0: Tesla P100-PCIE-16GB (UUID: GPU-aa9372c0-5736-d2ba-a5e4-29eaa5d4070f)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Step 1: uninstall conflicting packages first\n!pip uninstall -y pyarrow datasets transformers\n!pip uninstall -y pyarrow datasets fastparquet\n\n# Step 2: install compatible versions (single line)\n!pip install -q pyarrow==19.0.0 datasets==2.14.7 transformers==4.35.2 accelerate==0.24.1 peft==0.7.1 sentencepiece evaluate\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:04.155349Z","iopub.execute_input":"2025-12-17T10:50:04.155614Z","iopub.status.idle":"2025-12-17T10:50:17.923409Z","shell.execute_reply.started":"2025-12-17T10:50:04.155579Z","shell.execute_reply":"2025-12-17T10:50:17.922655Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pyarrow 19.0.0\nUninstalling pyarrow-19.0.0:\n  Successfully uninstalled pyarrow-19.0.0\nFound existing installation: datasets 2.14.7\nUninstalling datasets-2.14.7:\n  Successfully uninstalled datasets-2.14.7\nFound existing installation: transformers 4.35.2\nUninstalling transformers-4.35.2:\n  Successfully uninstalled transformers-4.35.2\n\u001b[33mWARNING: Skipping pyarrow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping fastparquet as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Imports and device setup\nimport os\nimport re\nimport math\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport torch\nfrom datasets import Dataset, DatasetDict\nimport evaluate\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nmetric = evaluate.load(\"accuracy\") \n# Device (torch) — Trainer uses this internally but it's useful\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\nMODEL_NAME = \"google/byt5-small\"  # small version (recommended start)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)  # ByT5 uses byte-level tokenization\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\nprint(\"Loaded model:\", MODEL_NAME)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:17.924903Z","iopub.execute_input":"2025-12-17T10:50:17.925736Z","iopub.status.idle":"2025-12-17T10:50:50.829314Z","shell.execute_reply.started":"2025-12-17T10:50:17.925688Z","shell.execute_reply":"2025-12-17T10:50:50.828479Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n2025-12-17 10:50:24.215103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765968624.361858     254 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765968624.408916     254 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1765968624.779161     254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1765968624.779202     254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1765968624.779205     254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1765968624.779207     254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986aee726c694b5487143cf8d21baa7a"}},"metadata":{}},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b370efaa952a4d14a13cd653278330c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1133cf1460174b3881b3cae9545c16ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4476d78e78e4280bf396754f6cd1fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a37442afc7445e85c27b134ffb1fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92cc84033e8e449b8169155a526b4ad1"}},"metadata":{}},{"name":"stdout","text":"Loaded model: google/byt5-small\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# importance of normalization : Convert everything to the standard Arabic set so the model learns consistently.\n# Cell 4: Unicode diacritics removal and normalization helpers\n\n# Arabic diacritics (Tashkeel) common ranges we remove:\n\nimport re\nimport unicodedata\n\n# Arabic diacritics (Harakat)\nDIACRITICS = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n\ndef remove_diacritics(text: str) -> str:\n    \"\"\"Remove Arabic diacritics (tashkeel) from text.\"\"\"\n    return DIACRITICS.sub(\"\", text)\n\ndef normalize_text_advanced(text):\n    # ---------------------------------------------------------\n    # 0. Unicode Normalize (very important)\n    # ---------------------------------------------------------\n    text = unicodedata.normalize(\"NFKC\", text)\n\n    # ---------------------------------------------------------\n    # 1. Fix common Arabic ligatures (ﻻ → لا)\n    # ---------------------------------------------------------\n    text = text.replace(\"\\uFEFB\", \"لا\").replace(\"\\uFEFC\", \"لا\")  # Lam-Alef ligatures\n\n    # ---------------------------------------------------------\n    # 2. Remove Tatweel\n    # ---------------------------------------------------------\n    text = text.replace(\"ـ\", \"\")\n\n    # ---------------------------------------------------------\n    # 3. Normalize letter shapes\n    # ---------------------------------------------------------\n    replacements = {\n        \"\\u0622\": \"ا\",  # Alef with madda\n        \"\\u0623\": \"ا\",  # Alef with hamza above\n        \"\\u0625\": \"ا\",  # Alef with hamza below\n        \"\\u0671\": \"ا\",  # Alef wasla\n        \"\\u0649\": \"ي\",  # Alef Maqsura → Yeh\n        \"\\u06CC\": \"ي\",  # Persian Yeh → Arabic Yeh\n        \"\\u0643\": \"ك\",  # Normalize Kaf\n        \"\\u06A9\": \"ك\",  # Persian Kaf → Arabic Kaf\n    }\n    for k, v in replacements.items():\n        text = text.replace(k, v)\n\n    # ---------------------------------------------------------\n    # 4. Remove page number references like ( 21 / 227 )\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\(\\s*\\d+\\s*/\\s*\\d+\\s*\\)\", \" \", text)\n\n    # ---------------------------------------------------------\n    # 5. Remove pure numeric brackets (e.g., ( 48 ))\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\(\\s*\\d+\\s*\\)\", \" \", text)\n\n    # ---------------------------------------------------------\n    # 6. Remove content in brackets when it is clearly commentary\n    #    Example: ( قَوْلُهُ : كَفَى )\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\(\\s*[^()]*?\\d+[^()]*?\\)\", \" \", text)  # commentary w/ numbers\n    # Optional: remove ALL bracketed commentary (if you want)\n    # text = re.sub(r\"[\\(\\[][^()\\[\\]]*[\\)\\]]\", \" \", text)\n\n    # ---------------------------------------------------------\n    # 7. Remove double diacritics (very important)\n    # ---------------------------------------------------------\n    text = re.sub(f\"({DIACRITICS})+\", r\"\\1\", text)\n\n    # ---------------------------------------------------------\n    # 8. Remove weird invisible characters\n    # ---------------------------------------------------------\n    invisible_chars = [\n        \"\\u200C\",  # Zero-width non-joiner\n        \"\\u200D\",  # Zero-width joiner\n        \"\\u200E\",  # Left-to-right mark\n        \"\\u200F\",  # Right-to-left mark\n        \"\\u202A\", \"\\u202B\", \"\\u202C\", \"\\u202D\", \"\\u202E\",\n    ]\n    for ch in invisible_chars:\n        text = text.replace(ch, \"\")\n\n    # ---------------------------------------------------------\n    # 9. Fix spacing around punctuation\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\s+([.,؛،؟])\", r\"\\1\", text)  # no space before punctuation\n    text = re.sub(r\"([.,؛،؟])\\s*\", r\"\\1 \", text)  # one space after punctuation\n\n    # ---------------------------------------------------------\n    # 10. Collapse multiple spaces\n    # ---------------------------------------------------------\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    return text.strip()\n\n# Quick sanity test:\nsample = \"وَلَوْ جَمَعَ ثُمَّ عَلِمَ\"\nprint(\"orig:\", sample)\nprint(\"no diac:\", remove_diacritics(sample))\nprint(\"norm:\", normalize_text_advanced(remove_diacritics(sample)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:50.831550Z","iopub.execute_input":"2025-12-17T10:50:50.832439Z","iopub.status.idle":"2025-12-17T10:50:50.851328Z","shell.execute_reply.started":"2025-12-17T10:50:50.832394Z","shell.execute_reply":"2025-12-17T10:50:50.850556Z"}},"outputs":[{"name":"stdout","text":"orig: وَلَوْ جَمَعَ ثُمَّ عَلِمَ\nno diac: ولو جمع ثم علم\nnorm: ولو جمع ثم علم\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n# Cell 5: Load files and create pairs (input = undiacritized, target = original)\n# Put your train.txt and val.txt into the Kaggle working directory or dataset input path.\n\n\nTRAIN_PATH = \"/kaggle/input/train-nlp-project/train.txt\"\nVAL_PATH = \"/kaggle/input/val-nlp-project/val.txt\"\n\ndef load_pairs_from_file(path):\n    inputs = []\n    targets = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            target = line\n            src = remove_diacritics(line)\n            src = normalize_text_advanced(src)\n            # optional: add bos/eos tokens? ByT5's tokenizer handles that\n            inputs.append(src)\n            targets.append(target)\n    return inputs, targets\n\ntrain_srcs, train_tgts = load_pairs_from_file(TRAIN_PATH)\nval_srcs, val_tgts = load_pairs_from_file(VAL_PATH)\n\nprint(\"Loaded samples:\", len(train_srcs), \"train,\", len(val_srcs), \"val\")\nprint(\"example input -> target:\")\nprint(train_srcs[0])\nprint(train_tgts[0])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:50.853813Z","iopub.execute_input":"2025-12-17T10:50:50.854052Z","iopub.status.idle":"2025-12-17T10:50:55.421152Z","shell.execute_reply.started":"2025-12-17T10:50:50.854029Z","shell.execute_reply":"2025-12-17T10:50:55.420487Z"}},"outputs":[{"name":"stdout","text":"Loaded samples: 50000 train, 2500 val\nexample input -> target:\nولو جمع ثم علم ترك ركن من الاولي بطلتا ويعيدهما جامعا، او من الثانية، فان لم يطل تدارك، والا فباطلة ولا جمع، ولو جهل اعادهما لوقتيهما\nوَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ، أَوْ مِنْ الثَّانِيَةِ ، فَإِنْ لَمْ يَطُلْ تَدَارَكَ ، وَإِلَّا فَبَاطِلَةٌ وَلَا جَمَعَ ، وَلَوْ جَهِلَ أَعَادَهُمَا لِوَقْتَيْهِمَا\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nfinal_inputs = train_srcs \nfinal_targets = train_tgts \n\n# Cell 6: Make HuggingFace datasets\ntrain_dataset = Dataset.from_dict({\n    \"input_text\": final_inputs,\n    \"target_text\": final_targets\n})\n\nval_dataset = Dataset.from_dict({\n    \"input_text\": val_srcs,\n    \"target_text\": val_tgts\n})\n\ndatasets = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset\n})\n\nprint(datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:55.422004Z","iopub.execute_input":"2025-12-17T10:50:55.422265Z","iopub.status.idle":"2025-12-17T10:50:55.736829Z","shell.execute_reply.started":"2025-12-17T10:50:55.422241Z","shell.execute_reply":"2025-12-17T10:50:55.736122Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 50000\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text'],\n        num_rows: 2500\n    })\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# # Cell 8: Tokenize examples for seq2seq training\n# # We need to tokenize inputs and targets; set reasonable max lengths.\n\nMAX_INPUT_LENGTH = 512  # ByT5 uses bytes; 512 is safe for sentences\nMAX_TARGET_LENGTH = 512\n\n    \ndef preprocess_batch(examples):\n    # examples: dict with lists input_text, target_text\n    model_inputs = tokenizer(\n        examples[\"input_text\"],\n        max_length=MAX_INPUT_LENGTH,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    # Tokenize targets (labels)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"target_text\"],\n            max_length=MAX_TARGET_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\n# Step 1: tokenize inputs and targets\ntokenized = datasets.map(preprocess_batch, batched=True, remove_columns=datasets[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:50:55.737872Z","iopub.execute_input":"2025-12-17T10:50:55.738246Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f90e4307ed4d969050e20286664532"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"all_labels = []\nfor example in tokenized[\"train\"]:\n    all_labels.extend(example[\"labels\"])\n\nprint(\"Max label ID:\", max(all_labels))\nprint(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Data collator and metrics\n\nfrom transformers import DataCollatorForSeq2Seq\nfrom evaluate import load   \n# no more load_metric\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100  # this is important\n)\n\n\n# Load WER metric from evaluate\nwer_metric = load(\"wer\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: DER implementation (character-level)\n# DER = (# incorrect diacritic characters predicted) / (# characters that should have diacritics) * 100\n\ndef calc_der_for_pair(gold: str, pred: str):\n    \"\"\"Compute counts for one pair: returns (total_diacritics_in_gold, wrong_diacritics_predicted).\"\"\"\n    # We'll iterate over gold and pred by base characters.\n    # Simpler approach: strip diacritics to get base char sequence, then align.\n    gold_bases = remove_diacritics(gold)\n    pred_bases = remove_diacritics(pred)\n\n    # If base lengths differ, align via a simple heuristic (we'll fallback to char-level min length)\n    L = min(len(gold_bases), len(pred_bases))\n    total_diac = 0\n    wrong = 0\n\n    gi = 0\n    pi = 0\n    # iterate over positions in bases (simple alignment)\n    for k in range(L):\n        # find indices in original strings that correspond to the k-th base char\n        # This is a little code-heavy but robust enough.\n        # Build lists of base char indices once (optimization skipped)\n        pass\n\n# Simpler robust implementation: expand each string into sequence of (base_char, diacritic_str)\ndef expand_to_bases_and_diacritics(s: str):\n    base_chars = []\n    diacritic_strs = []\n    current_base = None\n    current_diacs = []\n    for ch in s:\n        if DIACRITICS.match(ch):\n            if current_base is not None:\n                current_diacs.append(ch)\n            # else stray diacritic before base? ignore\n        else:\n            # new base char\n            if current_base is not None:\n                base_chars.append(current_base)\n                diacritic_strs.append(\"\".join(current_diacs))\n            current_base = ch\n            current_diacs = []\n    if current_base is not None:\n        base_chars.append(current_base)\n        diacritic_strs.append(\"\".join(current_diacs))\n    return base_chars, diacritic_strs\n\ndef der_counts(gold: str, pred: str):\n    gb, gd = expand_to_bases_and_diacritics(gold)\n    pb, pd = expand_to_bases_and_diacritics(pred)\n    L = min(len(gb), len(pb))\n    total_diac = 0\n    wrong = 0\n    for i in range(L):\n        gold_di = gd[i]\n        pred_di = pd[i]\n        if gold_di != \"\":\n            total_diac += 1\n            if gold_di != pred_di:\n                wrong += 1\n    # if gold has extra chars beyond pred, count their diacritics as errors\n    if len(gb) > L:\n        for i in range(L, len(gb)):\n            if gd[i] != \"\":\n                total_diac += 1\n                wrong += 1\n    return total_diac, wrong\n\n# Quick check\ng = \"وَلَوْ جَمَعَ\"\np = \"ولو جمع\"\nprint(\"DER counts:\", der_counts(g, p))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Training arguments\n# Adjust batch size depending on GPU:\n# - If T4: per_device_train_batch_size = 4 (or 2) \n# - If P100/V100: can try 8 or 16 for small\n# We'll use gradient accumulation to simulate larger effective batch.\n\nGPU_INFO = !nvidia-smi -L || true\ngpu_str = \"\\n\".join(GPU_INFO)\nprint(\"GPU:\", gpu_str)\n\n# Heuristics:\nif \"V100\" in gpu_str or \"A100\" in gpu_str or \"P100\" in gpu_str:\n    per_device_train_batch_size = 4\nelse:\n    per_device_train_batch_size = 2  # safer for T4\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"byt5_diacritizer\",\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_train_batch_size,\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,        # evaluate every 500 steps (tune for dataset size)\n    logging_steps=100,\n    save_total_limit=3,\n    save_steps=500,\n    num_train_epochs=5,\n    learning_rate=3e-4,    # ByT5 often likes 1e-4 - 5e-4; tune if needed\n    weight_decay=0.01,\n    warmup_steps=500,\n    fp16=False,             # use mixed precision if GPU supports\n    bf16=False,    \n    gradient_accumulation_steps=4,  # increase effective batch\n    gradient_checkpointing=True,           # Saves ~30 to 40% memory\n    load_best_model_at_end=True,\n    metric_for_best_model=\"char_acc\",\n    greater_is_better=True,\n    eval_accumulation_steps=4,\n    generation_max_length=MAX_TARGET_LENGTH,\n    report_to=\"none\",\n)\nprint(training_args)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: compute_metrics function for Trainer\nimport numpy as np\nfrom transformers import EvalPrediction\n\ndef postprocess_text(preds, labels):\n    # Labels may contain -100. Replace with pad token id for decoding\n    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n    # decode token ids to strings\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    return decoded_preds, decoded_labels\n\ndef compute_metrics(eval_preds):\n    # eval_preds: (pred_ids, labels_ids, (maybe) metrics)\n    preds_ids, labels_ids = eval_preds\n    if isinstance(preds_ids, tuple):\n        preds_ids = preds_ids[0]\n    # decode\n    decoded_preds, decoded_labels = postprocess_text(preds_ids, labels_ids)\n    \n    # compute simple token-level metrics: wer\n    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    \n    # compute DER\n    total_diac = 0\n    wrong_diac = 0\n    total_chars = 0\n    correct_chars = 0\n    for g, p in zip(decoded_labels, decoded_preds):\n        td, wd = der_counts(g, p)\n        total_diac += td\n        wrong_diac += wd\n        # simple char accuracy (consider base chars only)\n        gb, gd = expand_to_bases_and_diacritics(g)\n        pb, pd = expand_to_bases_and_diacritics(p)\n        L = min(len(g_bases), len(p_bases))\n        for i in range(L):\n            total_chars += 1\n            if g_bases[i] == p_bases[i] and ( (gd := expand_to_bases_and_diacritics(g)[1][i]) == (pd := expand_to_bases_and_diacritics(p)[1][i]) ):\n                correct_chars += 1\n    der = 100.0 * wrong_diac / total_diac if total_diac > 0 else 0.0\n    char_acc = 100.0 * correct_chars / total_chars if total_chars > 0 else 0.0\n\n    return {\n        \"wer\": wer,\n        \"DER\": der,\n        \"char_acc\": char_acc,\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example = tokenized['train'][0]  # pick first example\nprint(\"Input IDs type:\", type(example['input_ids']))\nprint(\"Input IDs length:\", len(example['input_ids']))\nprint(\"Labels length:\", len(example['labels']))\ndecoded_input = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\ndecoded_label = tokenizer.decode(example['labels'], skip_special_tokens=True)\nprint(\"Decoded input:\", decoded_input)\nprint(\"Decoded target:\", decoded_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.pad_token, tokenizer.pad_token_id)\nprint(tokenized[\"train\"][1][\"labels\"])\n\nprint(tokenizer.unk_token_id)\nprint(len(tokenizer))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: Create Seq2SeqTrainer and train\n\noutput_dir = \"/kaggle/working/byt5_diacritizer\"\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Auto resume if a checkpoint already exists\n# Automatically resumes if a checkpoint exists in output_dir\ntrainer.train()\n\n\n# Save final state\ntrainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Save and quick test\nbest_model_path = \"byt5_diacritizer/best_model\"\ntrainer.save_model(best_model_path)\nprint(\"Saved best model to\", best_model_path)\n\n# Simple inference function\nfrom transformers import pipeline\npipe = pipeline(\"text2text-generation\", model=best_model_path, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef diacritize(text: str, max_length: int = 512):\n    src = normalize_text_advanced(remove_diacritics(text))\n    out = pipe(src, max_length=max_length, do_sample=False)[0][\"generated_text\"]\n    return out\n\n# Example:\nprint(\"Input:\", \"ولو جمع ثم علم ترك ركن\")\nprint(\"Output:\", diacritize(\"وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}