{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install transformers==4.45.0 accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335192c1",
   "metadata": {},
   "source": [
    "## AraBERT (Frozen) + BiLSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports & configuration\n",
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "TRAIN_PATH = \"dataset/train.txt\"\n",
    "VAL_PATH = \"dataset/val.txt\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Arabic diacritics & text utilities\n",
    "\n",
    "ARABIC_DIACRITICS = set([\n",
    "    \"\\u064b\",  # Fathatan\n",
    "    \"\\u064c\",  # Dammatan\n",
    "    \"\\u064d\",  # Kasratan\n",
    "    \"\\u064e\",  # Fatha\n",
    "    \"\\u064f\",  # Damma\n",
    "    \"\\u0650\",  # Kasra\n",
    "    \"\\u0651\",  # Shadda\n",
    "    \"\\u0652\",  # Sukun\n",
    "    \"\\u0670\",  # Superscript Alef\n",
    "])\n",
    "\n",
    "def is_diacritic(ch: str) -> bool:\n",
    "    return ch in ARABIC_DIACRITICS\n",
    "\n",
    "def is_arabic_letter(ch: str) -> bool:\n",
    "    if not (\"\\u0600\" <= ch <= \"\\u06FF\" or \"\\u0750\" <= ch <= \"\\u077F\"):\n",
    "        return False\n",
    "    if is_diacritic(ch):\n",
    "        return False\n",
    "    cat = unicodedata.category(ch)\n",
    "    return cat.startswith(\"L\")\n",
    "\n",
    "def strip_diacritics(text: str) -> str:\n",
    "    return \"\".join(ch for ch in text if not is_diacritic(ch))\n",
    "\n",
    "# Arabic linguistic categories\n",
    "SUN_LETTERS = set(\"تثدذرزسشصضطظنل\")\n",
    "MOON_LETTERS = set(\"ءأإابجحخعغفقكمهوي\")\n",
    "\n",
    "# Common prefixes and suffixes\n",
    "ARABIC_PREFIXES = set(\"وفبكلس\")  # wa, fa, bi, ka, li, sa\n",
    "ARABIC_SUFFIXES = set(\"هاكني\")   # ha, ka, ni, ya (pronoun suffixes)\n",
    "\n",
    "# Special characters\n",
    "ALEF_VARIANTS = set(\"اأإآى\")\n",
    "WAW_YA = set(\"وي\")\n",
    "TA_MARBUTA = \"ة\"\n",
    "ALEF_MAQSURA = \"ى\"\n",
    "HAMZA_VARIANTS = set(\"ءأإؤئ\")\n",
    "\n",
    "print(\"Linguistic features defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Parse line with diacritic normalization\n",
    "\n",
    "def normalize_shadda_order(diacritics: List[str]) -> str:\n",
    "    \"\"\"Normalize: Shadda always comes first in combo.\"\"\"\n",
    "    if not diacritics:\n",
    "        return \"\"\n",
    "    shadda = \"\\u0651\"\n",
    "    has_shadda = shadda in diacritics\n",
    "    others = [d for d in diacritics if d != shadda]\n",
    "    if has_shadda:\n",
    "        return shadda + \"\".join(others)\n",
    "    return \"\".join(others)\n",
    "\n",
    "def process_line_to_bases_and_labels(line: str) -> Tuple[List[str], List[str]]:\n",
    "    line = line.rstrip(\"\\n\")\n",
    "    base_chars: List[str] = []\n",
    "    label_combos: List[str] = []\n",
    "\n",
    "    current_base = None\n",
    "    current_diacritics: List[str] = []\n",
    "\n",
    "    for ch in line:\n",
    "        if is_diacritic(ch):\n",
    "            if current_base is not None:\n",
    "                current_diacritics.append(ch)\n",
    "        else:\n",
    "            if current_base is not None:\n",
    "                combo = normalize_shadda_order(current_diacritics)\n",
    "                label_combos.append(combo)\n",
    "                base_chars.append(current_base)\n",
    "            current_base = ch\n",
    "            current_diacritics = []\n",
    "\n",
    "    if current_base is not None:\n",
    "        combo = normalize_shadda_order(current_diacritics)\n",
    "        label_combos.append(combo)\n",
    "        base_chars.append(current_base)\n",
    "\n",
    "    return base_chars, label_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ad8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build label vocabulary\n",
    "\n",
    "def build_label_vocab(path: str) -> Dict[str, int]:\n",
    "    combos = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            _, labels = process_line_to_bases_and_labels(line)\n",
    "            combos.update(labels)\n",
    "\n",
    "    normalized = set()\n",
    "    for c in combos:\n",
    "        normalized.add(\"NONE\" if c == \"\" else c)\n",
    "\n",
    "    sorted_labels = sorted(normalized, key=lambda x: (x != \"NONE\", x))\n",
    "    label2id = {lab: i for i, lab in enumerate(sorted_labels)}\n",
    "\n",
    "    print(\"Label set:\")\n",
    "    for i, lab in enumerate(sorted_labels):\n",
    "        print(f\"  {i}: {repr(lab)}\")\n",
    "    return label2id\n",
    "\n",
    "label2id = build_label_vocab(TRAIN_PATH)\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "NUM_LABELS = len(label2id)\n",
    "print(f\"\\nNUM_LABELS = {NUM_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49878d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Character vocabulary\n",
    "\n",
    "SPECIAL_TOKENS = [\"<PAD>\", \"<UNK>\"]\n",
    "\n",
    "def build_char_vocab(path: str) -> Dict[str, int]:\n",
    "    chars = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            base_chars, _ = process_line_to_bases_and_labels(line)\n",
    "            chars.update(base_chars)\n",
    "\n",
    "    sorted_chars = sorted(chars)\n",
    "    vocab = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "    for ch in sorted_chars:\n",
    "        if ch not in vocab:\n",
    "            vocab[ch] = len(vocab)\n",
    "\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "char2id = build_char_vocab(TRAIN_PATH)\n",
    "id2char = {i: ch for ch, i in char2id.items()}\n",
    "VOCAB_SIZE = len(char2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9119bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Line structure with word boundaries\n",
    "\n",
    "def line_to_struct(line: str):\n",
    "    base_chars, combos = process_line_to_bases_and_labels(line)\n",
    "    plain_text = \"\".join(base_chars)\n",
    "    words = plain_text.split()\n",
    "\n",
    "    char2word = []\n",
    "    current_word_idx = -1\n",
    "    inside_word = False\n",
    "\n",
    "    for ch in plain_text:\n",
    "        if ch.isspace():\n",
    "            char2word.append(-1)\n",
    "            if inside_word:\n",
    "                inside_word = False\n",
    "        else:\n",
    "            if not inside_word:\n",
    "                inside_word = True\n",
    "                current_word_idx += 1\n",
    "            char2word.append(current_word_idx)\n",
    "\n",
    "    return base_chars, combos, plain_text, words, char2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4beb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Enhanced linguistic feature extraction - 24 features per character\n",
    "\n",
    "def extract_enhanced_features(plain_text: str, char2word: List[int], words: List[str]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Extract 24 linguistic features per character.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    n = len(plain_text)\n",
    "    \n",
    "    # Precompute word info\n",
    "    word_starts = set()\n",
    "    word_ends = set()\n",
    "    pos = 0\n",
    "    for word in words:\n",
    "        word_starts.add(pos)\n",
    "        word_ends.add(pos + len(word) - 1)\n",
    "        pos += len(word) + 1  # +1 for space\n",
    "    \n",
    "    for i, ch in enumerate(plain_text):\n",
    "        f = []\n",
    "        \n",
    "        # === Basic type features (4) ===\n",
    "        f.append(1.0 if is_arabic_letter(ch) else 0.0)\n",
    "        f.append(1.0 if ch.isspace() else 0.0)\n",
    "        f.append(1.0 if ch.isdigit() else 0.0)\n",
    "        f.append(1.0 if unicodedata.category(ch).startswith(\"P\") else 0.0)\n",
    "        \n",
    "        # === Arabic letter categories (6) ===\n",
    "        f.append(1.0 if ch in SUN_LETTERS else 0.0)\n",
    "        f.append(1.0 if ch in MOON_LETTERS else 0.0)\n",
    "        f.append(1.0 if ch in ALEF_VARIANTS else 0.0)\n",
    "        f.append(1.0 if ch in HAMZA_VARIANTS else 0.0)\n",
    "        f.append(1.0 if ch in WAW_YA else 0.0)\n",
    "        f.append(1.0 if ch == TA_MARBUTA else 0.0)\n",
    "        \n",
    "        # === Position in word (4) ===\n",
    "        is_word_start = i in word_starts\n",
    "        is_word_end = i in word_ends\n",
    "        f.append(1.0 if is_word_start else 0.0)\n",
    "        f.append(1.0 if is_word_end else 0.0)\n",
    "        f.append(1.0 if is_word_start and is_word_end else 0.0)  # Single char word\n",
    "        \n",
    "        # Relative position in word\n",
    "        w_idx = char2word[i] if i < len(char2word) else -1\n",
    "        if w_idx >= 0 and w_idx < len(words):\n",
    "            word_len = len(words[w_idx])\n",
    "            # Find position within word\n",
    "            word_start_pos = sum(len(words[j]) + 1 for j in range(w_idx))\n",
    "            pos_in_word = i - word_start_pos\n",
    "            f.append(pos_in_word / max(word_len - 1, 1) if word_len > 1 else 0.5)\n",
    "        else:\n",
    "            f.append(0.0)\n",
    "        \n",
    "        # === Morphological hints (5) ===\n",
    "        # Prefix character (beginning of word)\n",
    "        f.append(1.0 if is_word_start and ch in ARABIC_PREFIXES else 0.0)\n",
    "        # Suffix character (end of word)\n",
    "        f.append(1.0 if is_word_end and ch in ARABIC_SUFFIXES else 0.0)\n",
    "        \n",
    "        # Definite article detection (ال)\n",
    "        is_alef_lam = False\n",
    "        if is_word_start and ch == 'ا' and i + 1 < n and plain_text[i + 1] == 'ل':\n",
    "            is_alef_lam = True\n",
    "        if i > 0 and plain_text[i - 1] == 'ا' and ch == 'ل' and (i - 1) in word_starts:\n",
    "            is_alef_lam = True\n",
    "        f.append(1.0 if is_alef_lam else 0.0)\n",
    "        \n",
    "        # After definite article (sun letter assimilation context)\n",
    "        after_al = False\n",
    "        if i >= 2 and w_idx >= 0:\n",
    "            word_start_pos = sum(len(words[j]) + 1 for j in range(w_idx))\n",
    "            if i - word_start_pos == 2:  # Third char in word\n",
    "                if plain_text[word_start_pos:word_start_pos+2] == \"ال\":\n",
    "                    after_al = True\n",
    "        f.append(1.0 if after_al else 0.0)\n",
    "        \n",
    "        # Ta Marbuta at word end (almost always Fatha)\n",
    "        f.append(1.0 if ch == TA_MARBUTA and is_word_end else 0.0)\n",
    "        \n",
    "        # === Context features (5) ===\n",
    "        # Previous character type\n",
    "        prev_ch = plain_text[i - 1] if i > 0 else ' '\n",
    "        f.append(1.0 if is_arabic_letter(prev_ch) else 0.0)\n",
    "        f.append(1.0 if prev_ch in ALEF_VARIANTS else 0.0)\n",
    "        \n",
    "        # Next character type\n",
    "        next_ch = plain_text[i + 1] if i + 1 < n else ' '\n",
    "        f.append(1.0 if is_arabic_letter(next_ch) else 0.0)\n",
    "        f.append(1.0 if next_ch.isspace() or i + 1 >= n else 0.0)  # Before space/end\n",
    "        f.append(1.0 if next_ch == TA_MARBUTA else 0.0)\n",
    "        \n",
    "        features.append(f)\n",
    "    \n",
    "    return features\n",
    "\n",
    "NUM_ENHANCED_FEATURES = 24\n",
    "\n",
    "# Test\n",
    "test_text = \"الكتاب\"\n",
    "test_base, _, test_plain, test_words, test_c2w = line_to_struct(test_text)\n",
    "test_feats = extract_enhanced_features(test_plain, test_c2w, test_words)\n",
    "print(f\"Features per char: {len(test_feats[0])}\")\n",
    "print(f\"Total chars: {len(test_feats)}\")\n",
    "assert len(test_feats[0]) == NUM_ENHANCED_FEATURES, f\"Expected {NUM_ENHANCED_FEATURES}, got {len(test_feats[0])}\"\n",
    "print(\"Feature extraction test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Dataset with enhanced features\n",
    "\n",
    "class EnhancedDiacritizationDataset(Dataset):\n",
    "    def __init__(self, path: str, char2id: Dict[str, int], label2id: Dict[str, int]):\n",
    "        self.samples = []\n",
    "        self.char2id = char2id\n",
    "        self.label2id = label2id\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                base_chars, combos, plain_text, words, char2word = line_to_struct(line)\n",
    "                \n",
    "                # Labels\n",
    "                multi_labels = []\n",
    "                binary_labels = []\n",
    "                mask = []\n",
    "                for ch, combo in zip(base_chars, combos):\n",
    "                    lab_name = \"NONE\" if combo == \"\" else combo\n",
    "                    if lab_name not in self.label2id:\n",
    "                        lab_name = \"NONE\"\n",
    "                    lab_id = self.label2id[lab_name]\n",
    "                    multi_labels.append(lab_id)\n",
    "                    binary_labels.append(0 if lab_name == \"NONE\" else 1)\n",
    "                    mask.append(1 if is_arabic_letter(ch) else 0)\n",
    "\n",
    "                char_ids = [self.char2id.get(ch, self.char2id[\"<UNK>\"]) for ch in base_chars]\n",
    "                \n",
    "                # Enhanced features\n",
    "                enhanced_feats = extract_enhanced_features(plain_text, char2word, words)\n",
    "\n",
    "                self.samples.append({\n",
    "                    \"char_ids\": char_ids,\n",
    "                    \"multi_labels\": multi_labels,\n",
    "                    \"binary_labels\": binary_labels,\n",
    "                    \"mask\": mask,\n",
    "                    \"plain_text\": plain_text,\n",
    "                    \"words\": words,\n",
    "                    \"char2word\": char2word,\n",
    "                    \"enhanced_feats\": enhanced_feats,\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "print(\"Enhanced dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Collate function with enhanced features\n",
    "\n",
    "def collate_fn_enhanced(batch):\n",
    "    max_len = max(len(sample[\"char_ids\"]) for sample in batch)\n",
    "    pad_id = char2id[\"<PAD>\"]\n",
    "\n",
    "    batch_char_ids = []\n",
    "    batch_multi = []\n",
    "    batch_binary = []\n",
    "    batch_mask = []\n",
    "    batch_plain_text = []\n",
    "    batch_words = []\n",
    "    batch_char2word = []\n",
    "    batch_enhanced_feats = []\n",
    "\n",
    "    for sample in batch:\n",
    "        L = len(sample[\"char_ids\"])\n",
    "        pad_len = max_len - L\n",
    "\n",
    "        batch_char_ids.append(sample[\"char_ids\"] + [pad_id] * pad_len)\n",
    "        batch_multi.append(sample[\"multi_labels\"] + [0] * pad_len)\n",
    "        batch_binary.append(sample[\"binary_labels\"] + [0] * pad_len)\n",
    "        batch_mask.append(sample[\"mask\"] + [0] * pad_len)\n",
    "        batch_char2word.append(sample[\"char2word\"] + [-1] * pad_len)\n",
    "\n",
    "        batch_plain_text.append(sample[\"plain_text\"])\n",
    "        batch_words.append(sample[\"words\"])\n",
    "        \n",
    "        # Pad enhanced features\n",
    "        feats = sample[\"enhanced_feats\"]\n",
    "        zero_feat = [0.0] * NUM_ENHANCED_FEATURES\n",
    "        padded_feats = feats + [zero_feat] * pad_len\n",
    "        batch_enhanced_feats.append(padded_feats)\n",
    "\n",
    "    return {\n",
    "        \"char_ids\": torch.tensor(batch_char_ids, dtype=torch.long),\n",
    "        \"multi_labels\": torch.tensor(batch_multi, dtype=torch.long),\n",
    "        \"binary_labels\": torch.tensor(batch_binary, dtype=torch.float32),\n",
    "        \"mask\": torch.tensor(batch_mask, dtype=torch.float32),\n",
    "        \"plain_text\": batch_plain_text,\n",
    "        \"words\": batch_words,\n",
    "        \"char2word\": torch.tensor(batch_char2word, dtype=torch.long),\n",
    "        \"enhanced_feats\": torch.tensor(batch_enhanced_feats, dtype=torch.float32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Create datasets and loaders\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = EnhancedDiacritizationDataset(TRAIN_PATH, char2id, label2id)\n",
    "val_dataset = EnhancedDiacritizationDataset(VAL_PATH, char2id, label2id)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_enhanced)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_enhanced)\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,} samples, {len(train_loader):,} batches\")\n",
    "print(f\"Val: {len(val_dataset):,} samples, {len(val_loader):,} batches\")\n",
    "\n",
    "# Test one batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  char_ids: {sample_batch['char_ids'].shape}\")\n",
    "print(f\"  enhanced_feats: {sample_batch['enhanced_feats'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Load AraBERT\n",
    "\n",
    "BERT_MODEL_NAME = \"aubmindlab/bert-base-arabertv02\"\n",
    "\n",
    "print(f\"Loading {BERT_MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
    "print(f\"BERT hidden size: {bert_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Enhanced model with refined feature integration\n",
    "\n",
    "class EnhancedDiacritizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model,\n",
    "                 vocab_size: int,\n",
    "                 num_labels: int,\n",
    "                 num_enhanced_feats: int = 24,\n",
    "                 emb_dim: int = 64,\n",
    "                 feat_hidden_dim: int = 48,  # Larger to handle 24 features\n",
    "                 lstm_hidden_dim: int = 256,\n",
    "                 lstm_layers: int = 2,  # Deeper LSTM\n",
    "                 dropout: float = 0.3,\n",
    "                 freeze_bert: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = bert_model\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        # Character embedding\n",
    "        self.char_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=char2id[\"<PAD>\"])\n",
    "\n",
    "        # Enhanced feature projection with more capacity\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(num_enhanced_feats, feat_hidden_dim),\n",
    "            nn.LayerNorm(feat_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(feat_hidden_dim, feat_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Input dimension\n",
    "        input_dim = emb_dim + feat_hidden_dim + self.bert_hidden_size\n",
    "\n",
    "        # Deeper BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.lstm_norm = nn.LayerNorm(lstm_hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Output heads\n",
    "        self.binary_head = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim * 2, lstm_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(lstm_hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.multi_head = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim * 2, lstm_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.Linear(lstm_hidden_dim, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        char_ids = batch[\"char_ids\"].to(DEVICE)\n",
    "        enhanced_feats = batch[\"enhanced_feats\"].to(DEVICE)\n",
    "        plain_text = batch[\"plain_text\"]\n",
    "        words_list = batch[\"words\"]\n",
    "        char2word = batch[\"char2word\"].to(DEVICE)\n",
    "\n",
    "        B, T = char_ids.shape\n",
    "\n",
    "        # 1) BERT word embeddings\n",
    "        encoding = tokenizer(\n",
    "            words_list,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_out = self.bert(**encoding)\n",
    "        token_embeddings = bert_out.last_hidden_state\n",
    "\n",
    "        # Average subword embeddings per word\n",
    "        word_embs_list = []\n",
    "        for i in range(B):\n",
    "            word_ids = encoding.word_ids(batch_index=i)\n",
    "            num_words = len(words_list[i])\n",
    "            H = token_embeddings.size(-1)\n",
    "            \n",
    "            if num_words == 0:\n",
    "                word_embs_list.append(torch.zeros((1, H), device=DEVICE))\n",
    "                continue\n",
    "                \n",
    "            sums = torch.zeros((num_words, H), device=DEVICE)\n",
    "            counts = torch.zeros((num_words, 1), device=DEVICE)\n",
    "\n",
    "            for tok_idx, w_id in enumerate(word_ids):\n",
    "                if w_id is not None and w_id < num_words:\n",
    "                    sums[w_id] += token_embeddings[i, tok_idx]\n",
    "                    counts[w_id] += 1.0\n",
    "\n",
    "            counts = torch.clamp(counts, min=1.0)\n",
    "            word_embs_list.append(sums / counts)\n",
    "\n",
    "        # Map to character level\n",
    "        bert_char_context = torch.zeros((B, T, self.bert_hidden_size), device=DEVICE)\n",
    "        for i in range(B):\n",
    "            word_embs = word_embs_list[i]\n",
    "            for j in range(T):\n",
    "                w_idx = char2word[i, j].item()\n",
    "                if 0 <= w_idx < word_embs.size(0):\n",
    "                    bert_char_context[i, j, :] = word_embs[w_idx]\n",
    "\n",
    "        # 2) Character embeddings\n",
    "        char_embs = self.char_emb(char_ids)\n",
    "\n",
    "        # 3) Enhanced feature projection\n",
    "        feat_proj = self.feat_proj(enhanced_feats)\n",
    "\n",
    "        # 4) Concatenate\n",
    "        x = torch.cat([char_embs, feat_proj, bert_char_context], dim=-1)\n",
    "\n",
    "        # 5) BiLSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.lstm_norm(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # 6) Heads\n",
    "        binary_logits = self.binary_head(lstm_out).squeeze(-1)\n",
    "        multi_logits = self.multi_head(lstm_out)\n",
    "\n",
    "        return binary_logits, multi_logits\n",
    "\n",
    "# Count parameters\n",
    "model = EnhancedDiacritizer(\n",
    "    bert_model=bert_model,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_labels=NUM_LABELS,\n",
    "    num_enhanced_feats=NUM_ENHANCED_FEATURES,\n",
    "    emb_dim=64,\n",
    "    feat_hidden_dim=48,\n",
    "    lstm_hidden_dim=256,\n",
    "    lstm_layers=2,\n",
    "    dropout=0.3,\n",
    "    freeze_bert=True\n",
    ").to(DEVICE)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Loss function\n",
    "\n",
    "def compute_loss(binary_logits, multi_logits, binary_labels, multi_labels, mask,\n",
    "                 lambda_binary=1.0, lambda_multi=1.0):\n",
    "    B, T = binary_logits.shape\n",
    "    C = multi_logits.shape[-1]\n",
    "\n",
    "    mask_flat = mask.view(-1)\n",
    "    mask_sum = mask_flat.sum() + 1e-8\n",
    "\n",
    "    # Binary loss\n",
    "    bce = nn.functional.binary_cross_entropy_with_logits(\n",
    "        binary_logits.view(-1), binary_labels.view(-1), reduction=\"none\"\n",
    "    )\n",
    "    bce = (bce * mask_flat).sum() / mask_sum\n",
    "\n",
    "    # Multi-class loss\n",
    "    ce = nn.functional.cross_entropy(\n",
    "        multi_logits.view(B * T, C), multi_labels.view(-1), reduction=\"none\"\n",
    "    )\n",
    "    ce = (ce * mask_flat).sum() / mask_sum\n",
    "\n",
    "    loss = lambda_binary * bce + lambda_multi * ce\n",
    "    return loss, bce.item(), ce.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Training configuration\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "LAMBDA_BINARY = 1.0\n",
    "LAMBDA_MULTI = 1.0\n",
    "GRAD_CLIP = 5.0\n",
    "\n",
    "print(\"Training config:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: 1e-3\")\n",
    "print(f\"  Weight decay: 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Training function\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, total_bce, total_ce = 0.0, 0.0, 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        binary_labels = batch[\"binary_labels\"].to(DEVICE)\n",
    "        multi_labels = batch[\"multi_labels\"].to(DEVICE)\n",
    "        mask = batch[\"mask\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        binary_logits, multi_logits = model(batch)\n",
    "        \n",
    "        loss, bce, ce = compute_loss(\n",
    "            binary_logits, multi_logits,\n",
    "            binary_labels, multi_labels, mask,\n",
    "            LAMBDA_BINARY, LAMBDA_MULTI\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_bce += bce\n",
    "        total_ce += ce\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / total_batches, total_bce / total_batches, total_ce / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Prediction and evaluation\n",
    "\n",
    "def predict_batch(model, batch, binary_threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        binary_logits, multi_logits = model(batch)\n",
    "        binary_probs = torch.sigmoid(binary_logits)\n",
    "        pred_multi_ids = multi_logits.argmax(dim=-1)\n",
    "        pred_binary = (binary_probs >= binary_threshold).long()\n",
    "        \n",
    "        none_id = label2id[\"NONE\"]\n",
    "        pred_multi_ids = pred_multi_ids.clone()\n",
    "        pred_multi_ids[pred_binary == 0] = none_id\n",
    "\n",
    "        return pred_binary.cpu(), pred_multi_ids.cpu()\n",
    "\n",
    "def evaluate(model, loader, binary_threshold=0.5):\n",
    "    model.eval()\n",
    "    total_chars, correct_chars = 0, 0\n",
    "    total_diac, wrong_diac = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            plain_texts = batch[\"plain_text\"]\n",
    "            gold_multi = batch[\"multi_labels\"].numpy()\n",
    "            mask = batch[\"mask\"].numpy()\n",
    "\n",
    "            _, pred_multi = predict_batch(model, batch, binary_threshold)\n",
    "            pred_multi = pred_multi.numpy()\n",
    "\n",
    "            B, T = gold_multi.shape\n",
    "            for i in range(B):\n",
    "                L = len(plain_texts[i])\n",
    "                for j in range(L):\n",
    "                    if mask[i, j] == 0:\n",
    "                        continue\n",
    "                    g_id, p_id = gold_multi[i, j], pred_multi[i, j]\n",
    "\n",
    "                    total_chars += 1\n",
    "                    if g_id == p_id:\n",
    "                        correct_chars += 1\n",
    "\n",
    "                    g_name = id2label[g_id]\n",
    "                    if g_name != \"NONE\":\n",
    "                        total_diac += 1\n",
    "                        if g_id != p_id:\n",
    "                            wrong_diac += 1\n",
    "\n",
    "    acc = 100.0 * correct_chars / total_chars if total_chars > 0 else 0.0\n",
    "    der = 100.0 * wrong_diac / total_diac if total_diac > 0 else 0.0\n",
    "    return acc, der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02419f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Main training loop\n",
    "\n",
    "best_der = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting training with enhanced features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_bce, train_ce = train_one_epoch(model, train_loader)\n",
    "    val_acc, val_der = evaluate(model, val_loader, binary_threshold=0.5)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}:\")\n",
    "    print(f\"  Train: loss={train_loss:.4f}, bce={train_bce:.4f}, ce={train_ce:.4f}\")\n",
    "    print(f\"  Val: acc={val_acc:.2f}%, DER={val_der:.2f}%\")\n",
    "\n",
    "    if val_der < best_der:\n",
    "        best_der = val_der\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        torch.save(best_state, \"enhanced_model_best.pt\")\n",
    "        print(f\"  ✓ New best DER: {best_der:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete! Best DER: {best_der:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc538a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Threshold optimization\n",
    "\n",
    "model.load_state_dict(torch.load(\"enhanced_model_best.pt\", map_location=DEVICE))\n",
    "\n",
    "print(\"Threshold sweep:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "best_thr = 0.5\n",
    "best_sweep_der = float(\"inf\")\n",
    "\n",
    "for thr in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "    acc, der = evaluate(model, val_loader, binary_threshold=thr)\n",
    "    marker = \" <-- best\" if der < best_sweep_der else \"\"\n",
    "    print(f\"  thr={thr:.2f}: acc={acc:.2f}%, DER={der:.2f}%{marker}\")\n",
    "    \n",
    "    if der < best_sweep_der:\n",
    "        best_sweep_der = der\n",
    "        best_thr = thr\n",
    "\n",
    "print(f\"\\nOptimal threshold: {best_thr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Demo diacritization\n",
    "\n",
    "def diacritize_text(model, text: str, threshold: float = 0.5) -> str:\n",
    "    model.eval()\n",
    "    \n",
    "    base_chars, _, plain, words, char2word = line_to_struct(text)\n",
    "    char_ids = [char2id.get(ch, char2id[\"<UNK>\"]) for ch in base_chars]\n",
    "    enhanced_feats = extract_enhanced_features(plain, char2word, words)\n",
    "\n",
    "    batch = {\n",
    "        \"char_ids\": torch.tensor([char_ids], dtype=torch.long),\n",
    "        \"enhanced_feats\": torch.tensor([enhanced_feats], dtype=torch.float32),\n",
    "        \"mask\": torch.ones((1, len(char_ids)), dtype=torch.float32),\n",
    "        \"plain_text\": [plain],\n",
    "        \"words\": [words],\n",
    "        \"char2word\": torch.tensor([char2word], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "    _, pred_multi = predict_batch(model, batch, threshold)\n",
    "    pred_ids = pred_multi[0].tolist()\n",
    "\n",
    "    out = []\n",
    "    for ch, lab_id in zip(text, pred_ids):\n",
    "        out.append(ch)\n",
    "        lab = id2label[lab_id]\n",
    "        if lab != \"NONE\":\n",
    "            out.append(lab)\n",
    "    return \"\".join(out)\n",
    "\n",
    "# Test examples\n",
    "test_sentences = [\n",
    "    \"ولو جمع ثم علم ترك ركن من الاولى بطلت\",\n",
    "    \"السلام عليكم ورحمة الله وبركاته\",\n",
    "    \"الحمد لله رب العالمين\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DIACRITIZATION DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    result = diacritize_text(model, sent, threshold=best_thr)\n",
    "    print(f\"\\nInput:  {sent}\")\n",
    "    print(f\"Output: {result}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
